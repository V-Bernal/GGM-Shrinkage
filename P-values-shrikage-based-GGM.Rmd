---
title: "P-values for shrinkage based GGM"
author: "Victor Bernal et al."
date: "Nov 12, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Description

This script reproduces the results published in  

* Title: EXACT HYPOTHESIS TESTING FOR SHRINKAGE BASED GGM.
* Publication: Bernal et al., Bioinformatics, https://doi.org/10.1093/bioinformatics/btz357 
* Author: Victor Bernal*, Rainer Bischoff, Victor Guryev, Marco Grzegorczyk, Peter Horvatovich
* Date created: 2018-11-12

Revision History  : Mar 09, 2019. Apr 26, 2020  
* Details: For the simulation of data and GGM estimation with the optimal shrinkage see [Sch?fer,J. and Strimmer,K. (2005a),  GeneNet 1.2.13. CRAN.]  
  
# Parameters
* p = Number of variables (e.g. genes)  
* n = Number of samples  
* number= Montecarlo iterations  
* rep = Times to repeat the simulation (for fixed p,n)  
* etaA = proportion of TP  
* alpha = significance level  
* eta0 = 1-etaA  

## Probability densities 
Here we compare qualitatively the standard vs the "shrunk" probability densities. As the shrinkage bias the partial correlation we expect that its density must account for the shrinkage value. Under the null hypothesis p-values are uniformly distributed in [0,1], we expect this bias will reflect on their distribution. 

# Load the libraries and functions
```{r ,  message = FALSE  }

library(GeneNet)
library(stats4)
library(Hmisc)
library(ggplot2)
source("shrinkagefunctions.R")

```

# Initialize the size of the network p, and the number of samples n 
```{r ,  message = FALSE  }
#***************************
# Initialize parameters

set.seed(123)
p<-100 # genes' number
etaA<-0.00 #proportion true positive correlations 
n<-c(30) # sample size
alpha<-0.05
number<-15 # MC iterations

```

# Simulate the network and data
```{r ,  message = FALSE  }
# Simulated pcorr and data
sim.pcor <-ggm.simulate.pcor(p, etaA)
sim.data <- ggm.simulate.data( n , sim.pcor)

# true and false positives
temp <- sim.pcor[lower.tri(sim.pcor, diag = FALSE) ]
TP <-which(temp!=0 )
TN <- which(temp==0 )

```
# Reconstruct/estimate the network of partial correlations
```{r ,  message = FALSE  }

# GGM with shrinkage
GGM <- pcor.shrink(sim.data, verbose=FALSE) # OPTIMAL
lambda <-attr(GGM, "lambda") 
#kappa(GGM, exact=TRUE, norm='2' )
r <-sm2vec(GGM)

# parameters of the mixture distribution used to compute p-values etc.
# c <- fdrtool(sm2vec(GGM), statistic="correlation")
# c$param
```
# Compute p-values
```{r ,  message = FALSE  }
# GENENET uses empirical null fitting with the standard density of the partial correlation
ENF.test<-network.test.edges(GGM, fdr=TRUE, plot=FALSE, verbose = FALSE)

# The test must be in node order. Sort by node 2 and by node 1
ENF.test<-ENF.test[order(ENF.test$node1,ENF.test$node2),]

# Shrunk MLE uses maximum likelihood and the new "shrunk" density
pval.shrunk<-p.shrunk(r,p,n,lambda)

# MLE (standard/std density)
# pval.std<-p.shrunk(r,p,n,0)
pval.standard <- p.standard ( r, p, n)

# Montecarlo is a non parametric method (computationally expensive)
p.monte<-p.montecarlo(r,number,p,n,lambda)
```
# Estimate the critical value using Montecarlo.
Beyond the critical value the partial correlation is significant  
```{r}

r.monte<-matrix(NA, length(r),number)
for (i in 1:number){
  r.data<-ggm.simulate.data(n,diag(p))
  r.monte.GGM<-ggm.estimate.pcor(r.data,lambda=lambda, verbose = FALSE )
  r.monte[,i]<-sm2vec(r.monte.GGM)
}
cv.values<-matrix(NA,1,ncol(r.monte))
for (i in 1:ncol(r.monte)){
  # 2 tails
  cv<-apply( -abs(r.monte),2, function(x) quantile(x,  probs = c((alpha/2)) ) )  
}
cv.monte<-mean(cv)   
cv.sd<-sd(cv)
```

# P-values should be uniformly distributed under the null-hypothesis. 
```{r ,  message = FALSE}
hist(p.monte,20, col=rgb(1,1,1,0.5),main = c(),xlab="p-values",ylim=c(0,max(320)),cex.axis=1,cex.lab=1)
hist(pval.shrunk,20, col=rgb(0.7,0.7,0.7,1),add=T)
hist(ENF.test$pval,20, col=rgb(0.3,0.3,0.3,1),add=T)
legend("bottomright", c("MC", "Shrunk MLE", "ENF"), fill=c("white","gray","gray21"), cex = 1, bg="white")#,bty="n"


hist(p.monte,20, col=rgb(1,1,1,0.5),main = c(),xlab="p-values",ylim=c(0,max(320)),cex.axis=1,cex.lab=1)
hist(pval.shrunk,20, col=rgb(0.7,0.7,0.7,1),add=T)
hist(pval.standard,20, col=rgb(0.3,0.3,0.3,1),add=T)
legend("bottomright", c("MC", "Shrunk MLE", "Standard MLE"), fill=c("white","gray","gray21"), cex = 1, bg="white")

```
Conclusion: we see that using the standard density (which does not account for the shrinkage) produces biased results.

```{r ,  message = FALSE}

###### Estimate degrees of freedom  k
#### Shrunk
sim.pcor.MC<-ggm.simulate.pcor(p, 0)
sim.data.MC<- ggm.simulate.data( n , sim.pcor.MC)
GGM.MC <- pcor.shrink(sim.data.MC,lambda , verbose=FALSE) 
kappa(GGM.MC, exact=TRUE, norm='2' )
r.MC<-sm2vec(GGM.MC)
nlogL.shrunk <- function(k) {
  
  density.shrunk <- function(r.MC) {(  ((1-lambda)^2-r.MC^2) ^((k-3)*0.5)  )/( beta(0.5, 0.5*(k-1))*(1-lambda)*(1-lambda)^(k-3) )}
  
  f<-density.shrunk(r.MC)
  -sum(log(f))
}
### neg log Likelihood
k.fit.shrunk <-  mle(nlogL.shrunk, start = list(k = 100), method = "L-BFGS-B", lower = c(20),
                     upper = c(Inf))
summary(k.fit.shrunk)


######## Standard
nlogL.std <- function(k) {
  density.std <- function(r.MC) {(  ((1)^2-r.MC^2) ^((k-3)*0.5)  )/( beta(0.5, 0.5*(k-1)))}
  
  f<-density.std(r.MC)
  
  -sum(log(f))
}

k.fit.std <-  mle(nlogL.std, start = list(k = 50), method = "L-BFGS-B", lower = c(20),
                  upper = c(Inf))
summary(k.fit.std)
```

Plot the difference of the densities when k is estimated via maximum likelihood for both, the "shrunk"" and the standard density.
```{r}
density.shrunk <- function(x) {(  ((1-lambda)^2-x^2) ^(( k.fit.shrunk@coef[1]-3)*0.5)  )/( beta(0.5, 0.5*( k.fit.shrunk@coef[1]-1))*(1-lambda)*(1-lambda)^( k.fit.shrunk@coef[1]-3) )}
density.std <- function(x) {(  ((1)^2-x^2) ^((k.fit.std@coef[1]-3)*0.5)  )/( beta(0.5, 0.5*(k.fit.std@coef[1])-1))}
x<-(1-lambda)*c(-100:100)/100


plot(x, density.std(x) - density.shrunk(x),ylim=c(-2,2),type="l",pch=20,lwd = 3, 
     ylab="Prob density",xlab="pcorr",cex=1.1, cex.lab=1.1, cex.axis=1.1 ) 
abline(v =0)
abline(h =0)
legend("bottomright", 
       legend = c("Standard MLE - Shrunk MLE", paste("critical value at",alpha)), 
       col = c("black", 
               "black"), 
       pch = c("-","."),
       bg = "white", 
       pt.cex = c(1,4), 
       cex = 1, 
       text.col = "black", 
       horiz = F , 
       inset = c(0, 0.1))
abline(v=c(cv.monte,-cv.monte), col=c("black","black"),lty=c("dotted", "dotted") , lwd=c(2,2))

```
Conclusion: we see that using the standard density has thicker tails.

Plot the difference of the densities when k is estimated from a empirical the mixture distribution  for the standard density (a.k.a. ENF), as implemented in the R package GeneNet.

```{r ,  message = FALSE}
c <- fdrtool(sm2vec(GGM), statistic="correlation", plot=FALSE)
c$param
data.frame( k.fit.shrunk@coef[1], k.fit.std@coef[1], c$param[5]) 

density.shrunk <- function(x) {(  ((1-lambda)^2-x^2) ^(( k.fit.shrunk@coef[1]-3)*0.5)  )/( beta(0.5, 0.5*( k.fit.shrunk@coef[1]-1))*(1-lambda)*(1-lambda)^( k.fit.shrunk@coef[1]-3) )}
density.std <- function(x) {(  ((1)^2-x^2) ^((c$param[5]-3)*0.5)  )/( beta(0.5, 0.5*(c$param[5])-1))}
x<-(1-lambda)*c(-100:100)/100

plot(x, density.std(x) - density.shrunk(x) ,type="l",ylim=c(-2,2), col="black",pch=20,
     lwd = 3, xlab="pcorr", cex=1.1, cex.lab=1.1, cex.axis=1.1, ylab="Prob density" )+
  axis(side = 2, at = c(1,1))+
  abline(v =0)
abline(h =0)
legend("bottomright", 
       legend = c("ENF - Shrunk MLE", paste("critical value at",alpha)), 
       col = c("black", 
               "black"), 
       pch = c("-","."),
       bg = "white", 
       pt.cex = c(1,4), 
       cex = 1, 
       text.col = "black", 
       horiz = F , 
       inset = c(0, 0.1))
abline(v=c(cv.monte,-cv.monte), col=c("black","black"),lty=c("dotted", "dotted"), lwd=c(2,2))
```
Conclusion: we see that using the standard density has thicker tails.

# Example 1: Eschericha coli

This data set describes the temporal expression of 102 genes of E. Coli after induction of the expression of SOD (recombinant human superoxide dismutase) from the Schmidt-Heck et al. (2004)
Essentially, this is a matrix with with 102 columns (=genes) and 9 rows (=time points).
All expression levels are given in log2-ratios with respect to the first time point (i.e. the induction at time 0).  

Source: The micoarray experiment was performed at the Institute of Applied Microbiology, University of Agricultural Sciences of Vienne. The data and the experiment is described in Schmidt-Heck et al.(2004)  


```{r}

library(GeneNet)
library(stats4)
library(Hmisc)
library(ggplot2)
#source("http://www.bioconductor.org/biocLite.R")
#class(biocLite)
#biocLite("limma")
library(limma)
#
set.seed(123)
alpha=0.05


# load data set
data(ecoli)

# how many samples and how many genes?
dim(ecoli)
summary(ecoli)
get.time.repeats(ecoli)
# plot first nine time series
plot(ecoli, 1:9)

#All expression levels are given in log2-ratios
number=40
n=nrow(ecoli)
ncol(ecoli)
pp<-ncol(ecoli)
```

# Reconstruct the network
```{r}
## GeneNet Strimmer
GGM_ecoli<-ggm.estimate.pcor(ecoli[,1:pp], method = c("static"))
lambda<-attr(GGM_ecoli,'lambda')
hist(sm2vec(GGM_ecoli))
kappa(GGM_ecoli)
r<-sm2vec(GGM_ecoli)

##### P values 

## ENF
test.ecoli <- network.test.edges(GGM_ecoli, plot = FALSE)
## MC
p.monte<-p.montecarlo(r,number,pp,n,lambda)
## Shrunk MLE
pval<-p.shrunk(r,pp,n,lambda)
```

# Compare the significat p values
```{r}
hist(test.ecoli$pval, col=rgb(1,1,1,0.3) ,20,main="",xlab="p-values")
hist(p.monte, col=rgb(0.1,0,0,0.3),20, add=T)
hist(pval, col=rgb(0.4,0.4,0.4,1),20, add=T)
legend("bottomright", c("ENF", "MC", "Shrunk MLE"), fill=c("white","gray","gray21"))
```
#p-values from our proposed method (MLE) and Montecarlo (MC) are similar.
```{r}
sum(test.ecoli$pval <=alpha)
sum(p.monte <=alpha)
sum(pval <=alpha)

barplot(  c(sum(test.ecoli$pval <=alpha) ,
            sum(pval <=alpha),
            sum(p.monte <=alpha)) ,col=c("gray","gray21","gray") ,names.arg=c("ENF", " Shrunk MLE","MC") )
legend("topright", c("ENF","Shrunk MLE", "MC"), col=c("gray","gray21","gray"), lwd=5)


abline(alpha*ncol(GGM_ecoli)*(ncol(GGM_ecoli)-1)*0.5,0)


test.ecoli<-test.ecoli[order(test.ecoli$node1,test.ecoli$node2), ]


a<- cbind(test.ecoli$pval<=alpha, pval<=alpha,p.monte<=alpha) 



vennDiagram(a , include = "both",
            names = c("ENF", "Shrunk MLE", "MC"), 
            cex = 0.5, counts.col = "black")
```
However, this small discrepancy that happens for small samples is magnified when the multiple testing correction is applied (e.g. Benjamini Hochberg or Bonferroni).
```{r}
# Significat BH adjusted p values
adj.strimmer<-p.adjust(   test.ecoli$pval , method = "BH", n = length(test.ecoli$pval)  )
adj.shrunk<-p.adjust(  pval, method = "BH", n = length(pval)  )
adj.mc<-p.adjust( p.monte ,method = "BH", n = length(p.monte ))

# barplot BH
barplot(  c(sum(adj.strimmer <=alpha) ,
            sum(adj.shrunk <=alpha),
            sum(adj.mc <=alpha)) ,col=c("gray","gray21","gray") ,names.arg=c("ENF", " Shrunk MLE","MC") )
legend("topright", c("ENF","Shrunk MLE", "MC"), col=c("gray","gray21","gray"), lwd=5)

abh<- cbind(adj.strimmer<=alpha, adj.shrunk<=alpha,adj.mc<=alpha) 

vennDiagram(abh, include = "both", 
            names = c("ENF", "Shrunk MLE", "MC"), 
            cex = 0.5, counts.col = "black")

adj.strimmer2<-p.adjust(   test.ecoli$pval , method = "bonferroni", n = length(test.ecoli$pval)  )
adj.shrunk2<-p.adjust(  pval, method = "bonferroni", n = length(pval)  )
adj.mc2<-p.adjust( p.monte ,method = "bonferroni", n = length(p.monte ))


barplot(  c(sum(adj.strimmer2 <=alpha) ,
            sum(adj.shrunk2 <=alpha),
            sum(adj.mc2 <=alpha)) ,col=c("gray","gray21","gray") ,names.arg=c("ENF", " Shrunk MLE","MC") )
legend("topright", c("ENF","Shrunk MLE", "MC"), col=c("gray","gray21","gray"), lwd=5)


abf<- cbind(adj.strimmer2<=alpha, adj.shrunk2<=alpha,adj.mc2<=alpha) 
vennDiagram(abf, include = "both", 
            names = c("ENF", "Shrunk MLE", "MC"), 
            cex = 0.5, counts.col = "black")
```

# Conclusions

The "shrunk" probability density accounts for the shrinkage's bias in the partial correlation. Under the null hypothesis, the "shrunk" p-values are uniformly distributed in [0,1] with the new density. The performance of the new approach "Shrunk MLE" is equivalent to MC (which is a un bias non-parametric test) while the computational time is kept low. This makes "Shrunk MLE" convinient for large scale applications e.g. genomics. For the E. coli example, the sample size n = 9 is very small and, while the p-values are similar to MC, any discrepancy is magnified when multiple testing adjustments is applied. We suggest the new test whenever n > 25.   